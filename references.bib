@article{2016arXiv160605250R,
  author        = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},
                   Konstantin and {Liang}, Percy},
  title         = {{SQuAD: 100,000+ Questions for Machine Comprehension of Text}},
  journal       = {arXiv e-prints},
  year          = 2016,
  eid           = {arXiv:1606.05250},
  pages         = {arXiv:1606.05250},
  archiveprefix = {arXiv},
  eprint        = {1606.05250}
}

@article{devlin2018bert,
  title   = {Bert: Pre-training of deep bidirectional transformers for language understanding},
  author  = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal = {arXiv preprint arXiv:1810.04805},
  year    = {2018}
}

@article{liu2019roberta,
  title   = {Roberta: A robustly optimized bert pretraining approach},
  author  = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal = {arXiv preprint arXiv:1907.11692},
  year    = {2019}
}

@article{rajpurkar2018know,
  title   = {Know what you don't know: Unanswerable questions for SQuAD},
  author  = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  journal = {arXiv preprint arXiv:1806.03822},
  year    = {2018}
}

@article{sanh2019distilbert,
  title   = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author  = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal = {arXiv preprint arXiv:1910.01108},
  year    = {2019}
}

@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}